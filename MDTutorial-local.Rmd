---
title: "Analysing physiological data collected during music listening: Pupillometry"
output: html_notebook
---

# Introduction

The eye's pupil dilates in response to increases in mental effort (i.e., the amount of attention and cognitive arousal that is evoked by a task). Under carefully controlled visual conditions, pupil diameter can be used as a psychophysiological index of attention. Pupil size can be captured using eye-tracking technology. Modern eye-trackers use cameras to measure the position and size of the pupil at a high frame rate.

In this part of the tutorial, we are going to explore eye-tracking data that were captured during a music discovery task.

## 0. Loading packages

```{r}
#install.packages("ggplot2", "FBN", "mgsub", "sciplot", "lme4", "lmerTest", "car", "fitdistrplus")
```

```{r}
lapply(c("ggplot2", "FBN", "mgsub", "sciplot", "lme4", "lmerTest", "car", "fitdistrplus"), require, character.only = TRUE)
```

## 1. Description of the data
  
We have created two files that contain subsets of the full dataset ("full-dataset.json") for use in this part of the tutorial.

*   **ET-partial1.txt** includes participant ID, session numbers, Spotify ID for each music track, track time (where 0 = start of the track), filtered pupil diameter, filtered and normalized pupil diameter, RSME ratings, liking ratings, and track familiarity.
*   **ET-partial2.txt** includes participant ID, session numbers, Spotify ID for each music track, track time (where 0 = start of the track), filtered pupil diameter, filtered and normalized pupil diameter, binocular gaze X and Y position data, and subjective emotion data (AOI hits in the Circumplex model).

We will use **ET-partial1.txt** to test the relationship between pupil size and RSME, liking, and familiarity, and **ET-partial2.txt** to test the relationship between pupil size and emotional response.

These files can be loaded into R using the read.table function. They will appear as data frames with variables listed in columns. Each row corresponds to a single observation of data for one participant/session/track.

```{r}
partial1 <- read.table("/Volumes/Current/Active_projects/MusicDiscovery/Analysis/MusicDiscoveryPupil/ET-partial1.txt")
head(partial1)
```

```{r}
partial2 <- read.table("/Volumes/Current/Active_projects/MusicDiscovery/Analysis/MusicDiscoveryPupil/ET-partial2.txt")
head(partial2)
```

Two other data files are associated with Section 2: "Pre-processing of pupil data".

*   **PLR.txt** contains a timestamp and pupil diameters for left and right eyes. This file contains data from a pupil light response task where a participant sat in darkness, and was periodically briefly presented with a bright light. The data can be plotted to show how the pupil responds to changes in lighting, but in this tutorial, we use it to illustrate examples of blinks.
*   **2067560969.txt** contains data from one participant, including participant ID, timestamp and track time, session number, gaze positions, ratings (liking, familiarity, RSME, fatigue, etc.). We use data from one trial for this participant to illustrate the pupil filtering procedure.

```{r}
plr <- read.table("/Volumes/Current/Active_projects/MusicDiscovery/Analysis/MusicDiscoveryPupil/PLR.txt", skip = 32, header = T, sep = ";")
head(plr)
```

```{r}
P2067560969 <- read.table("/Volumes/Current/Active_projects/MusicDiscovery/Analysis/MusicDiscoveryPupil/P2067560969.txt")
head(P2067560969)
```

## 2. Pre-processing of pupil data
### 2.1 Blinks
Pupil data normally contain blinks and other artifacts that should be removed before the data are used for any analysis. Below is an example of data containing blinks. A few samples are captured while the pupil is fully covered by the eyelid, resulting in a pupil size of 0 mm. A few samples are also captured while the eye is only partially covered by the eyelid. This results in a pupil size that is greater than 0, but much smaller than the average size for the trial. When pupil data are filtered, both complete and partial blinks need to be accounted for.

```{r}
blink <- read.table("/Volumes/Current/Active_projects/MusicDiscovery/Analysis/MusicDiscoveryPupil/PLR.txt", skip = 32, header = T, sep = ";")
blink$trialtime <- with(blink, (Time-Time[1])*(10^-6))
```

```{r}
options(repr.plot.width = 15, repr.plot.height = 6)
ggplot(blink,
aes(x = trialtime, y = R.Pupil.Diameter..mm.)) + geom_line() + geom_point(size = 3, colour = "blue") + xlim(30, 32) +
  xlab("Time (s)") + ylab("Right eye pupil diameter (mm)") +
  ggtitle("Example of pupil data with blinks")
```

### 2.2 Filtering pupil data
Some eye-tracking software includes a function for distinguishing fixations, saccades, and blinks:

*   **Fixations** occur when gaze is maintained on a single location
*   **Saccades** occur when gaze rapidly moves between locations
*   **Blinks** occur when the eye is fully or partially covered by the eyelid

Using such a function, you can get "clean" data for analysis by extracting only the fixations -- noting that fixations will not be evenly spaced in time. Fixation data is best for very detailed analysis of pupil dilations, because saccades (and blinks, of course) add noise to pupil data. If a blink detection function is not available or reliable, it may be necessary to work with raw data and filter out blinks and artifacts during pre-processing.

**Visualizing pupil data with blinks.** Below is an example of data from the Music Discovery dataset. Blinks are coded as NA in these data, so they appear as brief gaps. The "edges" of blinks appear as instantaneous drops in pupil size. There is also a longer period of missing data towards the end of the trial (probably because the participant looked away from the eye-tracker).

**Differences in size between left and right pupils.** Note that the left and right pupils follow similar trajectories, but are not identical in diameter. There is commonly a small difference between eyes in recorded pupil size, which is partially attributable to physiological differences between the eyes, and also sometimes partially attributable to measurement error (especially if the participant looks at more extreme angles or moves their head). Typically, pupillometry studies either use data from one eye or average data across both eyes.

```{r}
eyetracking <- read.table("/Volumes/Current/Active_projects/MusicDiscovery/Analysis/MusicDiscoveryPupil/P2067560969.txt")
```

```{r}
session.no <- 1
track <- "3c879SgICdtgNLUdzHHs8F"
tdata <- eyetracking[eyetracking$session == session.no & eyetracking$spotify_id == track, ]
```

```{r}
raw.plot <- ggplot(tdata, aes(x = tracktime)) + #xlim(30,40) +
                geom_line(aes(y = Pupil.diameter.right..mm., colour = "Right")) +
                geom_line(aes(y = Pupil.diameter.left..mm., colour = "Left")) +
                scale_colour_manual("", breaks = c("Right", "Left", "Smoothed", "Interpolated"),
                                    values = c("black", "blue", "red", "orange")) +
                ggtitle("Raw pupil data containing partial blinks") +
                xlab("Time (s)") + ylab("Pupil diameter (mm)")
raw.plot
```

We run a filter on these data that includes several steps. This is a customizable filter that is intended to work with data collected under different conditions using different eye-tracking devices. Depending on the quality of the data and the aim of the analyses, some steps might be skipped (e.g., smoothing). It might also be necessary to change some of the parameters. In this demo, we run steps 2-6, and the filter outputs a single continuous (i.e., without gaps) vector of smoothed pupil data.

* **Step 1**: *We skip this step here because the data do not contain values of 0.* Some data contain pupil sizes of 0, which arise during complete blinks or in moments where the pupil cannot be detected--for example, because the participant looks away from the eyetracker. Some software might code these values as NA (missing data) instead of 0. If values of 0 are present in the data, the first step would be to exclude them and replace them with NAs.
* **Step 1**: Filter out extreme velocities for each pupil individually. This targets the "edges" of blinks and other sudden changes in pupil size that arise because of mis-tracking.
* **Step 2**: Average left and right pupils.
* **Step 3**: Filter out extreme low values. Mostly these are samples from partial blinks that have excaped the velocity filter.
* **Step 4**: Run a linear interpolation to fill gaps. You can use other types of interpolations here. For example, a spline interpolation might be prefered in some cases (depending on how the data look, what size of gaps you need to fill, and what your eventual analysis will be). Here, we have some large gaps in some trials and prefer to maintain the average (tonic) pupil size in those cases.
* **Step 5**: Run a median filter to smooth the data.

```{r}
outlier.value <- 3 # Used for defining outlier velocities in pupil filtering
low.threshold <- 2 # Used for cutting off values that are too far below trial mean
window.size <- 17 # Used to set the smoothing window
```

```{r}
# Filter out extreme velocities for left pupil
tdata$xmm_velLeft <- with(tdata, c(NA, diff(Pupil.diameter.left..mm.))/c(NA, diff(timestamp)))
out.boundLeft <- outlier.value * sd(tdata$xmm_velLeft, na.rm = T)
tdata$xmm_ppLeft <- with(tdata, ifelse(abs(xmm_velLeft) > out.boundLeft, NA, Pupil.diameter.left..mm.))

# Filter out extreme velocities for right pupil
tdata$xmm_velRight <- with(tdata, c(NA, diff(Pupil.diameter.right..mm.))/c(NA, diff(timestamp)))
out.boundRight <- outlier.value * sd(tdata$xmm_velRight, na.rm = T)
tdata$xmm_ppRight <- with(tdata, ifelse(abs(xmm_velRight) > out.boundRight, NA, Pupil.diameter.right..mm.))

# Average filtered left and right pupils, then filter out extreme low values
tdata$pupil_lr <- with(tdata, (xmm_ppLeft + xmm_ppRight)/2)
low.bound <- mean(tdata$pupil_lr, na.rm = T) - low.threshold * sd(tdata$pupil_lr, na.rm = T)
tdata$xmm_ppp <- with(tdata, ifelse(pupil_lr < low.bound, NA, pupil_lr))

# Interpolate to fill gaps
tdata$pupil <- with(tdata, approx(timestamp, xmm_ppp, xout = timestamp)$y)

# Smoothing
tdata$smooth.pupil <- medianFilter(tdata$pupil, windowSize = window.size)
```

```{r}
raw.plot <- raw.plot + geom_line(data = tdata, aes(y = smooth.pupil, colour = "Smoothed"))
#raw.plot <- raw.plot + geom_line(data = tdata, aes(y = pupil, colour = "Interpolated")) + xlim(0, 10)
suppressWarnings(print(raw.plot))
```

**Note on blink analysis.** Like pupil size, eye-blinks are a useful measure of mental effort. People tend to blink at a lower rate when cognitive load is high, but blink at a faster rate when arousal is high. Increased visual demands decrease blink rate, while fatigue increases blink rate.

### 3.1 How does pupil size relate to self-reported mental effort (RSME scale)?

During the experiment, pupil data were collected as participants listened to several music tracks. They also had a "baseline" listening condition where they heard only a metronome. We would expect that pupil size is larger during the more stimulating music conditions than during the metronome condition. We would also expect that pupil size is more variable in the music conditions than in the metronome conditions as the demands on mental effort are constantly changing when music is playing. These differences are confirmed below with bargraphs showing differences in average pupil size and pupil size variability in the music and metronome conditions.

As a first step, before plotting the data, we will remove the last 5 seconds from all trials: we do this because participants were asked to make an active response (pressing a button to end the trial) when they had decided how much they liked the music. This active response had a noticable effect on pupil size (as well as EDA, as we see in the other part of the tutorial) that could affect the mean and variability values that we extract from the data.

```{r}
filtered <- read.table("/Volumes/Current/Active_projects/MusicDiscovery/Analysis/MusicDiscoveryPupil/ET-partial1.txt")
```

```{r}
allshort <- data.frame(matrix(rep(NA, 9), nrow = 1))
for (i in unique(filtered$ID)) {
  for (s in unique(filtered[filtered$ID == i, c("session")])) {
    for (j in unique(filtered[filtered$ID == i & filtered$session == s, c("spotify_id")])) {
      trial <- filtered[filtered$ID == i & filtered$session == s & filtered$spotify_id == j, ]
      end.time <- max(trial$tracktime)
      short.trial <- trial[trial$tracktime < end.time-5, ]
      colnames(allshort) <- colnames(short.trial)
      allshort <- rbind(allshort, short.trial)
    }
  }
}
allshort <- allshort[-1, ]
```

```{r}
nrow(allshort)
nrow(filtered)
```

```{r}
allshort$trialtype <- with(allshort, ifelse(spotify_id == "metronome", "metronome", "song"))
```

```{r}
allshort.m <- aggregate(npupil ~ trialtype + ID + session + spotify_id, allshort, mean)
bargraph.CI(trialtype, npupil, data = allshort.m,
             main = "Mean pupil size per trial in metronome and song conditions",
             xlab = "Trial type", ylab = "Normalized pupil size")
```

```{r}
allshort.sd <- aggregate(npupil ~ trialtype + ID + session + spotify_id, allshort, sd)
bargraph.CI(trialtype, npupil, data = allshort.sd,
             main = "Standard deviation in pupil size per trial in metronome & song conditions",
             xlab = "Trial type", ylab = "Normalized pupil size", cex.main = .9)
```

The plots show that, as expected, mental effort was greater and more variable when participants heard music rather than a metronome.

During the experiment, participants rated their mental effort on the RSME at the end of each trial. Theoretically, subjective (RSME) and objective (pupil size) measures of mental effort should be positively related. We can test whether this is the case for our data, using a Spearman correlation.

```{r}
pupilxRSME <- aggregate(smooth.pupil ~ RSME + ID + spotify_id + session, data = allshort, mean)
pupilxRSME$ID <- factor(pupilxRSME$ID)
```

```{r}
options(repr.plot.width = 15, repr.plot.height = 6)
effort.noID <- ggplot(pupilxRSME, aes(x = smooth.pupil, y = RSME)) + geom_point()
effort.noID <- effort.noID + ggtitle("Comparing average pupil size and RSME per participant and track") +
                                    xlab("Average pupil size") + ylab("RSME rating")
effort.noID + geom_smooth(method = lm, se = FALSE)
```

```{r}
effort <- ggplot(pupilxRSME, aes(x = smooth.pupil, y = RSME, colour = ID)) + geom_point()
effort <- effort + ggtitle("Comparing average pupil size and RSME per participant and track") +
                          xlab("Average pupil size") + ylab("RSME rating")
effort + geom_smooth(method = lm, se = FALSE)
```

```{r}
with(pupilxRSME, cor.test(smooth.pupil, RSME, method = "spearman", exact = F))
```

**Conclusions.** From the plot, we see that some participants used more of the RSME scale than others. The correlation is slightly but non-significantly negative, in contrast to our hypothesis. A possible explanation is that participants could not readily apply the concept of mental effort to a task that was generally enjoyable and that they associate with relaxation (i.e., listening to music - see also the results below on emotional response).

### 3.2 How does pupil size relate to liking and familiarity?

Participants reported how much they liked each track and how familiar they were with each track on a scale of 1-7.

*   **Liking:** Looking at the distribution of liking scores, we see that there were many more positive ratings (between 5 - 7) than negative or neutral ratings (< 5), and no ratings of 1. This skewed distribution is not optimal for testing the relationship between pupil size and liking.
*   **Familiarity:** Participants used the entire scale, and tended to be unfamiliar with the music, at least during the first session.

#### 3.2.1 Distributions of liking and familiarity ratings

```{r}
dist <- aggregate(cbind(liking, track_familiarity) ~ ID + spotify_id + session, allshort, mean)
table(dist$liking)
```

```{r}
hist(dist$liking, xlim = c(1, 7))
```

```{r}
table(dist$track_familiarity)
```

```{r}
hist(dist$track_familiarity)
```

#### 3.2.2 Pupil response to music with high vs. low liking ratings

When we test for a potential relationship between pupil size and liking or familiarity, we should keep in mind that liking/familiarity ratings are ordinal data, so equal distance between points on the scale cannot be assumed. One way to handle this is to recode ratings into "low" and "high" categories, and test for a difference in pupil size between these groups. Below, this is demonstrated for liking ratings.

```{r}
# Add a new categorical liking variable
allshort$likingcat <- with(allshort, ifelse(liking < 5, "low", "high"))
allshort$likingcat <- as.factor(allshort$likingcat)
```

```{r}
# Calculate average pupil size per participant, session, and track
pupilxLikingcat <- aggregate(npupil ~ liking + likingcat + ID + spotify_id + session, allshort, mean)
```

```{r}
# Plot pupil size per liking category
#bargraph.CI(likingcat, npupil, data = pupilxLikingcat)
bargraph.CI(likingcat, npupil, group = session, data = pupilxLikingcat, legend = T,
            x.leg = 5.5)# Test also with session as grouping variable
```

```{r}
# Compare pupil size for highly liked vs. not well liked music using a t-test
t.test(pupilxLikingcat[pupilxLikingcat$likingcat == "low", c("npupil")],
pupilxLikingcat[pupilxLikingcat$likingcat == "high", c("npupil")], paired = F)
```

#### 3.2.3 Predicting pupil response from liking and familiarity with linear mixed effects models
The relationship between pupil size and liking and/or familiarity ratings can also be tested without recoding the variables. Here, we use linear mixed effects modelling to test whether liking and familiarity predict pupil size.

Linear mixed effects models (LMM) are useful for analyzing data with hierarchical structures (e.g., participants/conditions/stimuli). Models typically test the relationship between certain fixed effects and an outcome variable. Models can also include random effects, which account for variability in higher-level variables.

LMM can be run with iterative testing to find the model that best explains the data. LMM can also be used for testing specific predicted effects (similar to an ANOVA). In this case, the formulation of the model is defined by the design of the experiment and hypotheses.

In this analysis, we want to test for a potential effects of liking and familiarity on pupil size. We have already seen above that these variables change between sessions, so we might also account for effects of session. We have also already seen (in Section 3.1) that participants differed in their average pupil size. Participant ID can be defined as a random effect to account for these differences. The term "(1|ID)" will add a random intercept for each participant.

Multicollinearity can be a problem for LMM, as it becomes difficult to estimate the individual contributions of each predictor and increases the risk of overfitting. There are different techniques for determining whether variables are too strongly correlated to be included together in the same model:

*   Correlation analysis
*   Variance inflation factor (VIF) - measure of how much multicollinearity affects the variance of the estimated regression coefficients (VIF > 5 indicates critical multicollinearity)

```{r}
# Find average pupil size per participant, session, & track
pertrial <- aggregate(smooth.pupil ~ ID + spotify_id + session + liking + track_familiarity, allshort, mean)
```

```{r}
# Check the distribution of smooth.pupil - is a log transform appropriate?
plot(fitdist(log(pertrial$smooth.pupil), distr = "norm"))
```

```{r}
# Check the distribution of smooth.pupil with a log transform
shapiro.test(log(pertrial$smooth.pupil))
```

```{r}
# Create a new variable in pertrial for log-transformed pupil size
pertrial$log.pupil <- log(pertrial$smooth.pupil)
```

```{r}
# Test for a correlation between liking and track_familiarity
cor.test(pertrial$liking, pertrial$track_familiarity, method = "spearman", exact = F)
```

```{r}
# Convert session to a factor and define contrasts
pertrial$session <- factor(pertrial$session)
contrasts(pertrial$session)<- c(-1, 1)
```

```{r}
# Run model 1, testing the effects of liking, track_familiarity, and session individually
summary(model1 <- lmer(log.pupil ~ liking + track_familiarity + session + (1|ID),
              REML = F, data = pertrial))
```

```{r}
# Check for multicollinearity
vif(model1)
```

```{r}
# Run model 2, testing also the interactions with session
summary(model2 <- lmer(log.pupil ~ liking*session + track_familiarity*session + (1|ID),
              REML = F, data = pertrial))
```

```{r}
# Compare the models
anova(model1, model2)
```

**Conclusions.** When we include liking, familiarity, and session as separate fixed effects in the model, there is a significant positive effect of liking on pupil size and a near-significant decrease in pupil size from session 1 to session 2. The effect of liking is maintained when we include the interactions between liking x session and familiarity x session, and the effect of session is also significant. 

### 3.3 Does pupil size predict listeners' emotional response?
Most eye-tracking software has a function for Area of Interest (AOI) analysis. Typically, the experimenter draws AOIs on the visual display that participants viewed during the experiment. The software function then run, and outputs "hits" for each sample (or fixation) where gaze position was recorded in any of the defined AOIs.

In our experiment, participants viewed the Circumplex model while listening to each track. They were instructed to look at the term(s) that best represented their emotional response to the music. They could look at different terms, one after another, if their emotional state changed as they listened. This allowed us to get a real-time measure of their subjective emotional experience during the listening task.

#### 3.3.1 Percentage of experiment time spent looking at emotion terms
An important question to address before analysing these data is how much time participants actually spend looking at one or another of the AOIs. We test this below by summing up the total number of data samples in which an AOI hit was recorded, and taking this sum as a percentage of the total number of data samples:

```{r}
aoi.data.full <- read.table("/Volumes/Current/Active_projects/MusicDiscovery/Analysis/MusicDiscoveryPupil/ET-partial2.txt")
```

```{r}
# Remove the last 5 seconds from all trials
allshort <- data.frame(matrix(rep(NA, 28), nrow = 1))
for (i in unique(aoi.data.full$ID)) {
  for (s in unique(aoi.data.full[aoi.data.full$ID == i, c("session")])) {
    for (j in unique(aoi.data.full[aoi.data.full$ID == i & aoi.data.full$session == s, c("spotify_id")])) {
      trial <- aoi.data.full[aoi.data.full$ID == i & aoi.data.full$session == s & aoi.data.full$spotify_id == j, ]
      end.time <- max(trial$tracktime)
      short.trial <- trial[trial$tracktime < end.time-5, ]
      colnames(allshort) <- colnames(short.trial)
      allshort <- rbind(allshort, short.trial)
    }
  }
}
allshort <- allshort[-1, ]
```

```{r}
# Create a new variable indicating whether or not there was a hit
AOI.only <- allshort[, grepl("AOI", colnames(allshort))]
AOI.only$sum.of.hits <- rowSums(AOI.only)
AOI.only[1000:1006,]
```

```{r}
# Calculate percentage of observations where there was a hit
mean(AOI.only$sum.of.hits, na.rm = T)*100
```

Across all participants and trials, only about 54% of listening time was spent with gaze fixed on one or another of the emotion terms. This is perhaps unsurprising, as participants might have needed some time to listen to the music and reflect before selecting an emotion. In the plot below, we show the position of all gaze data collected during the experiment relative to the Circumplex model. Non-AOI data is in dark blue, and AOI hits are in in light blue.

```{r}
# Subset observations where there is a hit
aoi.data.full$hit <- rowSums(aoi.data.full[, grepl("AOI", colnames(aoi.data.full))])
hits <- aoi.data.full[aoi.data.full$hit == 1, ]
```

```{r}
# Plot gaze position of all observations (dark blue) with hits in light blue & estimate position of emotion terms
with(aoi.data.full,
     plot(Gaze.point.X, Gaze.point.Y, col = "navy", bty = "n", cex.axis = .6, cex.lab = .6))
with(hits,
     points(Gaze.point.X, Gaze.point.Y, col = "dodgerblue"))
text(c(.5, .5, .8, .19), c(.9, .1, .5, .5),
     label = c("ACTIVATION", "DEACTIVATION", "PLEASANT", "UNPLEASANT"),
     cex = .6)
text(c(.62, .67, .72, .77, .77, .72, .67, .59, .38, .30, .25, .21, .21, .26, .32, .4),
     c(.83, .77, .69, .62, .40, .29, .21, .15, .15, .21, .29, .40, .62, .69, .77, .83),
     label = c("alert", "excited", "elated", "happy", "contented", "serene", "relaxed", "calm",
               "fatigued", "lethargic", "depressed", "sad", "upset", "stressed", "nervous", "tense"),
     cex = .6)
```

When participants were not looking at one or another of the emotion terms, where were they looking instead? The answer seems to be everywhere: their gaze was mostly inside the circle, where they were probably looking while they decided on their response. We can see from the plot that participants spent overall more time looking at the right side of the circle (pleasant emotions).

#### 3.3.2 Which emotions did participants experience while listening?
Below, we calculate how "hits" were distributed across the different emotion terms on the Circumplex model. The plot below represents the distribution of emotional responses that arose across the participant sample during the experiment.

```{r}
# Calculate number of hits per emotion term
AOI.only.nosum <- AOI.only[!is.na(AOI.only$AOI.hit..Pleasant.),!grepl("sum", colnames(AOI.only))]
sum.per.emotion <- colSums(AOI.only.nosum, na.rm = T)
number.of.hits <- nrow(AOI.only[which(AOI.only$sum.of.hits == 1), ])
```

```{r}
# Plot number of hits per emotion as a percentage of total hits
plot(sum.per.emotion/number.of.hits * 100, xaxt = "n",
      pch = 19, 
      xlab = "Emotions", ylab = "% total samples",
      main = "Listeners' emotional responses to music")
axis(1, at = 1:20, labels = mgsub(colnames(AOI.only.nosum), c("AOI.hit", "\\."), c("", "")), cex = .2)
abline(v = c(4.5, 8.5, 12.5, 16.5), lty = 2, col = "grey", lwd = 2)
```

```{r}
# Create new variables indicating where there are hits in each quadrant
aoi.data.full$quadActPleas <- rowSums(aoi.data.full[,c(9, 10, 13:16)])
aoi.data.full$quadDeactPleas <- rowSums(aoi.data.full[,c(10, 11, 17:20)])
aoi.data.full$quadDeactUnpleas <- rowSums(aoi.data.full[,c(11, 12, 21:24)])
aoi.data.full$quadActUnpleas <- rowSums(aoi.data.full[,c(9, 12, 25:28)])
```

```{r}
# Reformat data so that hits per quadrant are in long format
quad.df <- data.frame(ID = aoi.data.full$ID,
                      spotify_id = aoi.data.full$spotify_id,
                      session = aoi.data.full$session,
                      smooth.pupil = aoi.data.full$smooth.pupil,
                      npupil = aoi.data.full$npupil,
                      quadsums = c(aoi.data.full$quadActPleas, aoi.data.full$quadDeactPleas,
                                  aoi.data.full$quadDeactUnpleas, aoi.data.full$quadActUnpleas),
                      quadlabel = c(rep("ActPleas", nrow(aoi.data.full)), rep("DeactPleas", nrow(aoi.data.full)),
                                    rep("DeactUnpleas", nrow(aoi.data.full)), rep("ActUnpleas", nrow(aoi.data.full))))
quad.hit <- quad.df[quad.df$quadsums == 1, ]
```

```{r}
# Plot hits per quadrant as a percentage of total hits
bargraph.CI(quadlabel, quadsums*100, group = session, data = quad.df, legend = TRUE,
            xlab = "Circumplex quadrant", ylab = "Percentage of total AOI hits",
            x.leg = 10, y.leg = 25, leg.lab = c("Session 1", "Session 2"))
```

```{r}
# Calculate an average pupil size per participant, track, session, and quadrant
pupilxQuad <- aggregate(npupil ~ quadlabel + ID + spotify_id + session, quad.hit, mean,
                        na.action = na.pass, na.rm = T)
```

```{r}
# Plot pupil size per quadrant
bargraph.CI(quadlabel, npupil, data = pupilxQuad,
            xlab = "Circumplex quadrant", ylab = "Pupil size")
```

```{r}
# Create a new variable to code high vs. low activation
pupilxQuad$Activation <- with(pupilxQuad, ifelse(grepl("Act", quadlabel), "Act", "Deact"))
```

```{r}
# T-test to compare pupil size in high activation/pleasant and high activation/unpleasant responses
t.test(pupilxQuad[pupilxQuad$quadlabel == "ActPleas", c("npupil")],
       pupilxQuad[pupilxQuad$quadlabel == "ActUnpleas", c("npupil")])
```

```{r}
# T-test to compare pupil size in high activation/pleasant and low activation/pleasant responses
t.test(pupilxQuad[pupilxQuad$quadlabel == "ActPleas", c("npupil")],
       pupilxQuad[pupilxQuad$quadlabel == "DeactPleas", c("npupil")])
```

```{r}
# T-test to compare pupil size in high activation/pleasant and low activation/unpleasant responses
t.test(pupilxQuad[pupilxQuad$quadlabel == "ActPleas", c("npupil")],
       pupilxQuad[pupilxQuad$quadlabel == "DeactUnpleas", c("npupil")])
```

**Conclusions.** Participants' emotional response to the music they heard was mostly positive: most AOI hits were in the pleasant-deactivation quadrant, followed by the pleasant-activation quadrant. Notably, hits were more evenly distributed between the two quadrants in the second listening session. Pupil size was largest when participants experienced pleasant activation, and smallest when participants experienced unpleasant deactivation. The effect of activation is in line with the idea that pupil dilation occurs during cognitive arousal. The added effect of pleasant emotions suggest that participants might have been more engaged by music that elicited a pleasant emotional response.

# Optional tutorial tasks

## 3.2 How does pupil size relate to liking and familiarity?

For this task, you will need ET-partial1.txt, and we recommend dropping the last seconds of each trial. 

```{r}
filtered <- read.table("/Volumes/Current/Active_projects/MusicDiscovery/Analysis/MusicDiscoveryPupil/ET-partial1.txt")
```

```{r}
allshort <- data.frame(matrix(rep(NA, 9), nrow = 1))
for (i in unique(filtered$ID)) {
  for (s in unique(filtered[filtered$ID == i, c("session")])) {
    for (j in unique(filtered[filtered$ID == i & filtered$session == s, c("spotify_id")])) {
      trial <- filtered[filtered$ID == i & filtered$session == s & filtered$spotify_id == j, ]
      end.time <- max(trial$tracktime)
      short.trial <- trial[trial$tracktime < end.time-5, ]
      colnames(allshort) <- colnames(short.trial)
      allshort <- rbind(allshort, short.trial)
    }
  }
}
allshort <- allshort[-1, ]
```

* Task 1. For this analysis, we need a data frame with one row per trial and participant containing average pupil size, liking, and familiarity data. Use "ET-partial1.txt" for this (or "allshort" if you want to exclude the last 5 seconds of each trial). 

* Task 2. Check the distributions of the data and decide whether any transformation is needed.

* Task 3. Test for multicollinearity between factors. (This can also be done after running the model, using the package "car".)

* Task 4. Use lmer to define a model testing the effects of liking and familiarity on pupil size. 
  + Include a random intercept for participants. The fixed effects can also be included in the definition of the random effect for a more complex model.
  + You might also choose to include session number in the model (as its own effect and/or in an interaction). 
  + You can define multiple models and then use the "anova" function to compare how well they explain the data. 








